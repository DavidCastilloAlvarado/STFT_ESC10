{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import librosa.display\n",
    "import pylab\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV , RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score , plot_confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "import librosa    \n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "rcParams['figure.figsize'] = 20, 5\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"There are not GPUs avaliable\")\n",
    "# Auxiliar FUNC\n",
    "def plot_confusion_matrix_custom(title, classifier, X_test, y_test, class_names, normalize = None):\n",
    "    disp = plot_confusion_matrix(classifier, X_test, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = []\n",
    "labels = []\n",
    "sampling_rate = []\n",
    "file_names = []\n",
    "classes = []\n",
    "label_number=0\n",
    "audio_data = []\n",
    "labels = []\n",
    "sampling_rate = []\n",
    "file_names = []\n",
    "data = []\n",
    "noisy_removed=[]\n",
    "noise=[]\n",
    "samplerate = 44000\n",
    "max_leng=0 # mínima cantidad de muestras por sonido\n",
    "for filepath in glob.iglob('dataset/*'):\n",
    "    #print(filepath[9:])\n",
    "    #print(filepath)\n",
    "    classes.append(filepath[8:])\n",
    "print(classes)\n",
    "\n",
    "for i in classes:\n",
    "    print(\"the class = \"+i+\", the label = \"+str(label_number))\n",
    "    for j in glob.iglob('dataset/'+i+'/*'):\n",
    "        #samplerate, data = wavfile.read(j)\n",
    "        y, s = librosa.load(j, sr=samplerate) # Downsample 44.1kHz\n",
    "        #reduced_noise = nr.reduce_noise(audio_clip=y, noise_clip=y, verbose=False)\n",
    "        #print(s)\n",
    "        #print(j)\n",
    "        audio_data.append(y)\n",
    "        max_leng = len(y) if len(y)>max_leng else max_leng\n",
    "        #noise.append(y)\n",
    "        labels.append(label_number)\n",
    "\n",
    "    label_number = label_number + 1\n",
    "    print(\"Cantidad máxima de muestras / Sonido / clase: \", max_leng)\n",
    "print(\"Cantidad máxima de muestras / Sonido: \", max_leng)\n",
    "print(label_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALIZANDO UNA MUESTRA \n",
    "example = 306\n",
    "samples = audio_data[example]\n",
    "sound_duration = (len(samples)/samplerate)\n",
    "step = sound_duration/len(samples)\n",
    "print(\"Duración: \", sound_duration,\"s\")\n",
    "print(\"SampleTime: {:6f}s \".format(step))\n",
    "\n",
    "# Visualizando Sonido en el Tiempo vs Amplitud\n",
    "save_path= 'tiempoVSamp.jpg'\n",
    "print(\"Cantidad de muestras: \",len(samples))\n",
    "plt.plot([step*i for i in range(len(samples))],samples)\n",
    "plt.xlabel('Time s')\n",
    "plt.ylabel('Amplitud []')\n",
    "plt.title('signal in real time')\n",
    "pylab.savefig(save_path, bbox_inches=None, pad_inches=0)\n",
    "plt.show()\n",
    "#pylab.close()\n",
    "\n",
    "# Analizando Sonido ZTiempo vs Coef STFT\n",
    "time_init = time.time()\n",
    "rcParams['figure.figsize'] = 20, 5\n",
    "n_fft = 1024 # n_fft/2+1 como la cantidad de bandas a descomponer en el espectro de frecuencia\n",
    "win_length=1024 # Ventaneo de la STFT\n",
    "hop_length=int(win_length/2) # Desplazamiento de la ventana de transformación\n",
    "data_sound = audio_data[example] if len(audio_data[example]) == max_leng else np.pad(audio_data[example], (0,max_leng-len(audio_data[example])), constant_values = (0,0))\n",
    "freq = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data[example],n_fft=n_fft, win_length=win_length ,hop_length=hop_length)), ref=np.max) #Se obtiene la potencia de la transformada \n",
    "print(\"Dimención de la STFT: \", freq.shape)\n",
    "print(\"Clase: \",classes[labels[example]])\n",
    "print(\"Windowing Time: \", win_length*1/samplerate,\"s\")\n",
    "time_final = time.time()\n",
    "data = (time_final- time_init )/1000\n",
    "print(\"tiempo de ejecución STFT: {:2f} uS\".format(data*10**6))\n",
    "save_path2 = 'SFTF_'+save_path\n",
    "#plt.axis('off') # no axis\n",
    "librosa.display.specshow(freq,hop_length=hop_length,x_axis='time', y_axis='linear',sr=44000)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('short time fourier transform')\n",
    "pylab.savefig(save_path2, bbox_inches=None, pad_inches=0)\n",
    "#pylab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando STFT a todas las muestras de sonido\n",
    "X_stft = []\n",
    "Y_label=[]\n",
    "cant_samples = max_leng\n",
    "for i, i_sound in tqdm(enumerate(audio_data)):\n",
    "    i_sound = i_sound if len(i_sound) == cant_samples else np.pad(i_sound, (0,cant_samples-len(i_sound)), constant_values = (0,0))\n",
    "    coef_stft = librosa.amplitude_to_db(np.abs(librosa.stft(i_sound, n_fft=n_fft, win_length=win_length ,hop_length=hop_length)), ref=np.max)\n",
    "    stft_shape = coef_stft.shape\n",
    "    #freq = np.abs(librosa.stft(audio_data[i], n_fft=512, hop_length=256, win_length=512))\n",
    "    freq=coef_stft.reshape(-1,1)\n",
    "    X_stft.append(freq)\n",
    "    Y_label.append(labels[i])\n",
    "    # if freq.shape[0]>=cant_samples:\n",
    "    #     X_stft.append(freq[:cant_samples])\n",
    "    #     Y_label.append(labels[i])\n",
    "    \n",
    "X_stft =np.stack(X_stft) \n",
    "X_stft.shape\n",
    "X_stft=X_stft.reshape(len(audio_data),-1)\n",
    "X_stft.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando datos\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_stft,y = X_stft)\n",
    "normalized_stft = scaler.transform(X_stft)\n",
    "print(np.amax(X_stft))\n",
    "print(np.amax(normalized_stft))\n",
    "\n",
    "# Dividiendo lasmuestras de forma homogenea\n",
    "#from skmultilearn.model_selection import iterative_train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_stft,Y_label, test_size=0.20,random_state =2,stratify= Y_label)\n",
    "print(\"Split ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo con SVM\n",
    "clf = svm.SVC(verbose= True,random_state=150,C=10, kernel='linear', gamma='auto')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"SVM: acc:\"+ str(accuracy_score(y_test, y_pred)))\n",
    "plot_confusion_matrix_custom(title = \"SVM\", \n",
    "                            classifier=clf, \n",
    "                            X_test=X_test, \n",
    "                            y_test = y_test,\n",
    "                            class_names =  classes,\n",
    "                            normalize =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con PCA y SVM\n",
    "print(\"Cantidad de caracteristicas\", len(X_stft[0]))\n",
    "pca = PCA(n_components=320)\n",
    "pca.fit(normalized_stft) ## justa para todo el espectro de datos\n",
    "X_train_PCA = pca.transform(X_train)\n",
    "X_test_PCA = pca.transform(X_test)\n",
    "clf_PCA = svm.SVC(verbose= True,random_state=None,C=10, kernel='linear', gamma='scale', probability=True )\n",
    "clf_PCA.fit(X_train_PCA, y_train)\n",
    "y_pred = clf_PCA.predict(X_test_PCA)\n",
    "print(\"SVM+PCA: acc:\"+ str(accuracy_score(y_test, y_pred)))\n",
    "plot_confusion_matrix_custom(title = \"SVM+PCA\", \n",
    "                            classifier=clf_PCA, \n",
    "                            X_test=X_test_PCA, \n",
    "                            y_test = y_test,\n",
    "                            class_names =  classes,\n",
    "                            normalize =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo PCA & SVM & GRIDsearch\n",
    "#simple performance reporting function\n",
    "def clf_performance(classifier, model_name):\n",
    "    print(model_name)\n",
    "    print('Best Score: ' + str(classifier.best_score_))\n",
    "    print('Best Parameters: ' + str(classifier.best_params_))\n",
    "clf = svm.SVC(verbose= True,random_state=2)\n",
    "\n",
    "param_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': ['scale',.1,.5,1,5,10],\n",
    "                                  'C': [.1, 1, 10, 100, 1000]},\n",
    "                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n",
    "                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\n",
    "clf_svc = GridSearchCV(clf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = 4)\n",
    "best_clf_svc = clf_svc.fit(X_train_PCA, y_train)\n",
    "clf_performance(best_clf_svc,'SVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo clasificador con XGB & PCA\n",
    "xgb_model = XGBClassifier(learning_rate=0.01,\n",
    "                    n_estimators=1200,\n",
    "                    max_depth=100,\n",
    "                    min_child_weight=.05,\n",
    "                    gamma=0,\n",
    "                    subsample=.5,\n",
    "                    colsample_bytree=0.5,\n",
    "                    objective='multi:softmax',\n",
    "                    nthread=4,\n",
    "                    num_class=10,\n",
    "                    num_parallel_tree = 20,\n",
    "                    seed=27,verbosity= 1,n_jobs=8 )\n",
    "xgb_model.fit(X_train_PCA, y_train)\n",
    "#xgb_model.save_model('models/xgbmodel')\n",
    "y_pred = xgb_model.predict(X_test_PCA)\n",
    "print(\"XGB+PCA: acc:\"+ str(accuracy_score(y_test, y_pred)))\n",
    "plot_confusion_matrix_custom(title = \"XGB+PCA\", \n",
    "                            classifier=xgb_model, \n",
    "                            X_test=X_test_PCA, \n",
    "                            y_test = y_test,\n",
    "                            class_names =  classes,\n",
    "                            normalize =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparando datos para la CNN\n",
    "X_train_NN= np.reshape(X_train,(X_train.shape[0],stft_shape[0], -1,1))\n",
    "X_test_NN = np.reshape(X_test,(X_test.shape[0],stft_shape[0], -1,1))\n",
    "y_train_NN=keras.utils.to_categorical(y_train, num_classes=10, dtype='float32')\n",
    "y_test_NN =keras.utils.to_categorical(y_test, num_classes=10, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compra para compra de proyecto de IVA\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), input_shape=X_train_NN.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(32, (3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))#input_shape=features.shape[1:]\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(32))#input_shape=features.shape[1:]\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "#sgd = optimizers.SGD(lr=0.0000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "n_epochs = 0\n",
    "#tf.keras.utils.plot_model(model, to_file='NN_model.jpg', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "# Entrenando modelo Deep learning\n",
    "logdir=\"logs2\" \n",
    "epoch_add = 20\n",
    "tboard_callback = TensorBoard(log_dir=logdir)\n",
    "history = model.fit(X_train_NN, y_train_NN,\n",
    "                    #steps_per_epoch = 8,   #cantidad de veces que se calculará el gradiente |DATOStotale = steps_per_epoch * batch_size\n",
    "                    batch_size=8,          #cantidad de muestras para calcular el gradiente\n",
    "                    epochs=n_epochs+epoch_add,\n",
    "                    initial_epoch = n_epochs,\n",
    "                    callbacks=[tboard_callback],\n",
    "                    validation_data = (X_test_NN,y_test_NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0) \n",
    "plt.figure()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit6ea9d59176c94d2aba530003a360c2f8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
